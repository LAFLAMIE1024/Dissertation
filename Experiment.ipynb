{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, ShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, recall_score, precision_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import xgboost as xgb\n",
    "from ucimlrepo import fetch_ucirepo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "concrete_compressive_strength = fetch_ucirepo(id=45)\n",
    "\n",
    "original_df = concrete_compressive_strength.data.original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preproc(df):\n",
    "#     df = original_df.copy()\n",
    "#     df.loc[df[(df['num'] > 0)].index.tolist(),'num'] = 1\n",
    "\n",
    "#     dropped_df = df.dropna()\n",
    "\n",
    "#     X = dropped_df.drop('num', axis=1)\n",
    "#     y = dropped_df['num']\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     X_train = X_train.reset_index()\n",
    "#     X_test = X_test.reset_index()\n",
    "#     y_train = y_train.reset_index()\n",
    "#     y_test = y_test.reset_index()\n",
    "\n",
    "#     return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cv(X_train, y_train, md, fold):\n",
    "    \n",
    "#     param = {\n",
    "#         \"max_depth\": md, \n",
    "#         \"eta\": 1, \n",
    "#         \"objective\": \"binary:logistic\"\n",
    "#     }\n",
    "\n",
    "#     dtrain = xgb.DMatrix(\n",
    "#         data=X_train,\n",
    "#         label=y_train\n",
    "#     )\n",
    "    \n",
    "#     res = xgb.cv(\n",
    "#         param,\n",
    "#         dtrain,\n",
    "#         num_boost_round=10,\n",
    "#         nfold=fold,\n",
    "#         metrics=(\"error\"),\n",
    "#         seed=42,\n",
    "#         as_pandas = True\n",
    "\n",
    "#     )\n",
    "\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_acc(model, X_test, y_test, md):\n",
    "\n",
    "#     # Initialize a XGBoost model with tested parameters\n",
    "#     model = xgb.XGBClassifier(\n",
    "#         max_depth=md,\n",
    "#         n_estimators=10,\n",
    "#         eta=1,\n",
    "#         random_state = 42\n",
    "#     )\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     y_pred = model.predict(X_test)\n",
    "\n",
    "#     # Evaluate on the test set\n",
    "#     precision = precision_score(y_test, y_pred)\n",
    "#     recall    = recall_score(y_test, y_pred)\n",
    "#     f1        = f1_score(y_test, y_pred)\n",
    "#     accuracy  = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#     print(f'Accuracy: {accuracy * 100:.3f}%, Precision: {precision * 100:.3f}%, Recall: {recall * 100:.3f}%, F1: {f1 * 100:.3f}%')\n",
    "\n",
    "#     return y_test, y_pred, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest -> importance scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_dict(df):\n",
    "#     names = df.columns.to_list()\n",
    "\n",
    "#     names_dict = {}\n",
    "#     for i, n in enumerate(df.columns.to_list()):\n",
    "#         # print(f\"index: {i}, name: {n}\")\n",
    "#         names_dict[n] = i\n",
    "        \n",
    "#     num_li = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']\n",
    "#     cat_li = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "\n",
    "#     num_dict = {n:names_dict[n] for n in num_li}\n",
    "#     cat_dict = {c:names_dict[c] for c in cat_li}\n",
    "\n",
    "#     return names, num_dict, cat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestRegressor(n_estimators=20, max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.248, 'thal'), (0.233, 'cp'), (0.223, 'ca'), (0.16, 'exang'), (0.119, 'oldpeak'), (0.118, 'slope'), (0.104, 'thalach'), (0.083, 'trestbps'), (0.065, 'sex'), (0.041, 'age'), (0.038, 'chol'), (0.025, 'fbs'), (0.006, 'restecg')]\n"
     ]
    }
   ],
   "source": [
    "# scores = []\n",
    "\n",
    "# for i in range(X.shape[1]):\n",
    "\n",
    "#     # Select one feature at a time, do the cross validation\n",
    "#     score = cross_val_score(rf, X.iloc[:, i:i+1],y, scoring=\"r2\")\n",
    "\n",
    "#     \"\"\"\n",
    "#     The function ShuffleSplit() randomly samples from the data\n",
    "#     set for 10 times with the specified test set\n",
    "#     \"\"\"\n",
    "#     # Split the training and test set at 7:3\n",
    "#     cv = ShuffleSplit(n_splits=10, test_size=.3)\n",
    "#     scores.append((abs(round(np.mean(score), 3)), names[i]))\n",
    "\n",
    "# # Print out the sorted score of all features\n",
    "# print(sorted(scores, reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Cross Validation with parameter setting: max_depth=2, nfold=5}\n",
      "\\label{md2f5}\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & train-error-mean & train-error-std & test-error-mean & test-error-std \\\\\n",
      "\\midrule\n",
      "0 & 0.213116 & 0.015284 & 0.312500 & 0.073607 \\\\\n",
      "1 & 0.154013 & 0.011774 & 0.253546 & 0.068211 \\\\\n",
      "2 & 0.132938 & 0.020277 & 0.232181 & 0.048959 \\\\\n",
      "3 & 0.117104 & 0.010386 & 0.240248 & 0.054846 \\\\\n",
      "4 & 0.108688 & 0.027745 & 0.240248 & 0.068266 \\\\\n",
      "5 & 0.090738 & 0.019343 & 0.261525 & 0.054085 \\\\\n",
      "6 & 0.073851 & 0.014203 & 0.240337 & 0.065662 \\\\\n",
      "7 & 0.069635 & 0.009193 & 0.253103 & 0.047619 \\\\\n",
      "8 & 0.054871 & 0.008028 & 0.240337 & 0.065320 \\\\\n",
      "9 & 0.047480 & 0.009478 & 0.240426 & 0.072062 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# result = cv(X_train, y_train, md=2, fold=5)\n",
    "# print(result.to_latex(\n",
    "#     caption='Cross Validation with parameter setting: max_depth=2, nfold=5', label='md2f5',\n",
    "#     #float_format=\"{:.3f}\".format,\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Cross Validation with parameter setting: max_depth=2, nfold=10}\n",
      "\\label{md2f10}\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & train-error-mean & train-error-std & test-error-mean & test-error-std \\\\\n",
      "\\midrule\n",
      "0 & 0.217998 & 0.006554 & 0.290580 & 0.047986 \\\\\n",
      "1 & 0.160340 & 0.009601 & 0.239493 & 0.088017 \\\\\n",
      "2 & 0.141562 & 0.017076 & 0.223007 & 0.076283 \\\\\n",
      "3 & 0.126572 & 0.013973 & 0.197283 & 0.104060 \\\\\n",
      "4 & 0.110634 & 0.014944 & 0.201630 & 0.104973 \\\\\n",
      "5 & 0.095641 & 0.014272 & 0.201630 & 0.105266 \\\\\n",
      "6 & 0.090948 & 0.013113 & 0.188949 & 0.095844 \\\\\n",
      "7 & 0.084371 & 0.011910 & 0.209964 & 0.091015 \\\\\n",
      "8 & 0.072202 & 0.011553 & 0.184964 & 0.075213 \\\\\n",
      "9 & 0.062825 & 0.013641 & 0.202174 & 0.069146 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# result = cv(X_train, y_train, md=2, fold=10)\n",
    "# print(result.to_latex(\n",
    "#     caption='Cross Validation with parameter setting: max_depth=2, nfold=10', \n",
    "#     label='md2f10',\n",
    "#     #float_format=\"{:.3f}\".format,\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Cross Validation with parameter setting: max_depth=4, nfold=5}\n",
      "\\label{md4f5}\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & train-error-mean & train-error-std & test-error-mean & test-error-std \\\\\n",
      "\\midrule\n",
      "0 & 0.130833 & 0.021884 & 0.257801 & 0.069885 \\\\\n",
      "1 & 0.080184 & 0.012320 & 0.262057 & 0.076424 \\\\\n",
      "2 & 0.042211 & 0.008904 & 0.240869 & 0.055675 \\\\\n",
      "3 & 0.025324 & 0.007034 & 0.223936 & 0.048623 \\\\\n",
      "4 & 0.014759 & 0.009051 & 0.215160 & 0.048592 \\\\\n",
      "5 & 0.004216 & 0.003940 & 0.240426 & 0.030605 \\\\\n",
      "6 & 0.003163 & 0.002583 & 0.236170 & 0.035277 \\\\\n",
      "7 & 0.004222 & 0.005170 & 0.240248 & 0.037127 \\\\\n",
      "8 & 0.003163 & 0.002583 & 0.244415 & 0.049113 \\\\\n",
      "9 & 0.000000 & 0.000000 & 0.232004 & 0.039318 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# result = cv(X_train, y_train, md=4, fold=5)\n",
    "# print(result.to_latex(\n",
    "#     caption='Cross Validation with parameter setting: max_depth=4, nfold=5', label='md4f5',\n",
    "#     #float_format=\"{:.3f}\".format,\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Cross Validation with parameter setting: max_depth=2, nfold=5}\n",
      "\\label{md4f10}\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & train-error-mean & train-error-std & test-error-mean & test-error-std \\\\\n",
      "\\midrule\n",
      "0 & 0.131741 & 0.011004 & 0.252899 & 0.068775 \\\\\n",
      "1 & 0.084858 & 0.010381 & 0.256522 & 0.096598 \\\\\n",
      "2 & 0.054379 & 0.010053 & 0.248188 & 0.089339 \\\\\n",
      "3 & 0.035161 & 0.007894 & 0.231884 & 0.077370 \\\\\n",
      "4 & 0.022974 & 0.006785 & 0.232246 & 0.094362 \\\\\n",
      "5 & 0.015013 & 0.009342 & 0.232065 & 0.075047 \\\\\n",
      "6 & 0.008449 & 0.006230 & 0.236413 & 0.099977 \\\\\n",
      "7 & 0.004693 & 0.004199 & 0.236413 & 0.111610 \\\\\n",
      "8 & 0.001878 & 0.003114 & 0.240217 & 0.100804 \\\\\n",
      "9 & 0.000939 & 0.001878 & 0.231884 & 0.115764 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# result = cv(X_train, y_train, md=4, fold=10)\n",
    "# print(result.to_latex(\n",
    "#     caption='Cross Validation with parameter setting: max_depth=2, nfold=5', label='md4f10',\n",
    "#     #float_format=\"{:.3f}\".format,\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Cross Validation with parameter setting: max_depth=2, nfold=5}\n",
      "\\label{md6f5}\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & train-error-mean & train-error-std & test-error-mean & test-error-std \\\\\n",
      "\\midrule\n",
      "0 & 0.117093 & 0.018715 & 0.274557 & 0.067616 \\\\\n",
      "1 & 0.065408 & 0.018750 & 0.270301 & 0.056772 \\\\\n",
      "2 & 0.030593 & 0.011251 & 0.232181 & 0.052385 \\\\\n",
      "3 & 0.024294 & 0.014454 & 0.240603 & 0.056202 \\\\\n",
      "4 & 0.011607 & 0.006147 & 0.236259 & 0.040994 \\\\\n",
      "5 & 0.006338 & 0.005186 & 0.236348 & 0.036808 \\\\\n",
      "6 & 0.004222 & 0.002111 & 0.223493 & 0.048877 \\\\\n",
      "7 & 0.004216 & 0.003940 & 0.219326 & 0.050913 \\\\\n",
      "8 & 0.000000 & 0.000000 & 0.232004 & 0.042092 \\\\\n",
      "9 & 0.000000 & 0.000000 & 0.236170 & 0.042633 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# result = cv(X_train, y_train, md=6, fold=5)\n",
    "# print(result.to_latex(\n",
    "#     caption='Cross Validation with parameter setting: max_depth=2, nfold=5', label='md6f5',\n",
    "#     #float_format=\"{:.3f}\".format,\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Cross Validation with parameter setting: max_depth=2, nfold=5}\n",
      "\\label{md6f10}\n",
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      " & train-error-mean & train-error-std & test-error-mean & test-error-std \\\\\n",
      "\\midrule\n",
      "0 & 0.120480 & 0.012341 & 0.274094 & 0.064601 \\\\\n",
      "1 & 0.062814 & 0.008870 & 0.256884 & 0.076538 \\\\\n",
      "2 & 0.037034 & 0.008994 & 0.257065 & 0.091630 \\\\\n",
      "3 & 0.015941 & 0.007027 & 0.244203 & 0.084447 \\\\\n",
      "4 & 0.009846 & 0.004426 & 0.244022 & 0.090126 \\\\\n",
      "5 & 0.004693 & 0.004695 & 0.261413 & 0.086756 \\\\\n",
      "6 & 0.001406 & 0.002148 & 0.236232 & 0.091478 \\\\\n",
      "7 & 0.000937 & 0.001874 & 0.231884 & 0.105804 \\\\\n",
      "8 & 0.000469 & 0.001408 & 0.219203 & 0.095001 \\\\\n",
      "9 & 0.000937 & 0.001874 & 0.219022 & 0.090777 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# result = cv(X_train, y_train, md=6, fold=10)\n",
    "# print(result.to_latex(\n",
    "#     caption='Cross Validation with parameter setting: max_depth=2, nfold=5', label='md6f10',\n",
    "#     #float_format=\"{:.3f}\".format,\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.33%, Precision: 75.00%, Recall: 87.50%, F1: 80.77%\n"
     ]
    }
   ],
   "source": [
    "# base_y_test, base_y_pred, base_accuracy, base_precision, base_recall, base_f1 = evaluate_acc(X_test, y_test, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cm = confusion_matrix(y_test, base_y_pred)\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(2), yticklabels=range(2))\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Missing values (Implementation)\n",
    "\n",
    "The percentage of missing values goes from 10% to 50%. And missing values of all 3 kinds (Missing Completely At Random/Missing At Random/Missing Not At Randoma) are mixed.\n",
    "\n",
    "The way of creating missing values are described in the following subsections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MCAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_pair(length, width): # Input the length and width of the whole data frame\n",
    "#     return (np.random.randint(0, length), np.random.randint(0, width)) # generate a pair of numbers, which is the location of the missing entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def insert_mcar(ratio, df): # Input the percentage of mcar, and the data frame\n",
    "\n",
    "#     # Number of null values to insert\n",
    "#     num_nulls = int(ratio * (df.size))\n",
    "\n",
    "#     # Randomly choose positions to insert null values\n",
    "#     idx_pair = []\n",
    "\n",
    "#     for _ in range(num_nulls):\n",
    "        \n",
    "#         p = generate_pair(df.shape[0], df.shape[1])\n",
    "#         while p in idx_pair: p = generate_pair(df.shape[0], df.shape[1])\n",
    "#         idx_pair.append(p)\n",
    "        \n",
    "#         df.iat[p[0], p[1]] = np.nan\n",
    "    \n",
    "#     return df#, pd.isna(temp_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Missingness\n",
    "\n",
    "In the following work, we will call both MAR and MNAR the 'special missingness', as they provide much more information than MCAR and have not been fully explored. Meanwhile, 'target variable' refers to the column we chose to place artificial missing. The choice of target variable is worth investigating. On one hand, if we are removing something significantly contribute to the prediction of the outcome, which might lead to poor performance of the model. On the other hand, slight difference could be found if a relatively unrelated variable is chosen to be the target varialbe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MAR\n",
    "\n",
    "Suppose column 1 and 2 are selected for generating MAR, and we want to remove some values in column 1 based on the values appear on column 2. There are infinite way of creating missing values. Here we would exclude the output column 'num' from candidate variable as missingness in response variable will not be fed into the predictive model.\n",
    "\n",
    "Notice that we are handling a data set with mixed type of features (Integer/Categorical, all in form of number), and thus the choice of variable to be based on is vital. \n",
    "\n",
    "In this case, we are introducing MAR in column based on 'quantile' of integer variables.\n",
    "\n",
    "To be specific, say we need 6 percent data MAR of the whole data set, and if the value in column 'age' of an entry is smaller than 6th quantile of column 'age', then we remove the value in column 'trestbps'.\n",
    "\n",
    "Since the distribution of categorical variable determines the percentage of missing values, which means that it could not be adjusted for experimental purpose, we are not using any categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def insert_mar(ratio, df, dep_col, tar_col): \n",
    "    \n",
    "#     \"\"\"\n",
    "#     Input:\n",
    "#         - The percentage of data MAR to be generated\n",
    "#         - The data frame\n",
    "#         - Dependent column name: str\n",
    "#         - Index of the target column to be inserted\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Get the index of entries to be remvoed\n",
    "#     target_idx = df[dep_col] < df[dep_col].quantile(ratio)\n",
    "\n",
    "#     # Remove the entries in the target column\n",
    "#     df.iloc[target_idx, names_dict[tar_col]] = np.nan\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define MNAR\n",
    "\n",
    "Values in a certain column could be removed based on the values themselves. Similarly, if greater than 30 percentage of all the other values in this column, then turn them into null. Or, we can simply set up a known probability for creating missing values, and it is hidden from the downstream task model as the information the missingness of MNAR is depending on should remian unobservable. The way we creat missing values is straightforward, however, it could be much more complex in real world data set. Complicated or domain-specific design of generating missing values for investigation could be further developed to approximate the true situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def insert_mnar(ratio, df, tar_col): \n",
    "\n",
    "#     \"\"\"\n",
    "#     Input:\n",
    "#         - The percentage of data MNAR to be generated\n",
    "#         - The data frame\n",
    "#         - Target column name: str\n",
    "#         - Index of the target column to be inserted\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Get the index of entries to be remvoed\n",
    "#     target_idx = df[tar_col] < df[tar_col].quantile(ratio)\n",
    "    \n",
    "#     # Remove the entries in the target column\n",
    "#     df.iloc[target_idx, names_dict[tar_col]] = np.nan\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def insert_null(dataframe, ratio_mcar, ratio_mar, ratio_mnar):\n",
    "#     \"\"\"\n",
    "#         Input:\n",
    "#             - The data frame to be inserted\n",
    "#             - Ratios for each missing mechanism\n",
    "#     \"\"\"\n",
    "\n",
    "#     temp_df = dataframe.copy()\n",
    "    \n",
    "#     # Example\n",
    "#     mar_dep_col = \"ca\"\n",
    "#     mar_tar_col = \"thal\"\n",
    "#     temp_df = insert_mar(ratio_mcar, temp_df, mar_dep_col, mar_tar_col)\n",
    "\n",
    "#     mnar_tar_col = \"cp\"\n",
    "#     temp_df = insert_mnar(ratio_mnar, temp_df, mnar_tar_col)\n",
    "    \n",
    "#     temp_df = insert_mcar(ratio_mar, temp_df)\n",
    "    \n",
    "#     return temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting Null values\n",
    "\n",
    "We specify the total missing ratio and manually split it into 3 parts. Notice that due to the unpredictivity of MCAR, the actual missing ratio may be slightly lower than the expected percentage as there might be overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # total missing ratio = 0.1\n",
    "# ratio_mcar, ratio_mar, ratio_mnar = 1/30, 1/30, 1/30\n",
    "# missing_df = insert_null(df, ratio_mcar, ratio_mar, ratio_mnar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    }
   ],
   "source": [
    "# print(pd.isna(missing_df).sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation\n",
    "\n",
    "We start with the feature with least missing entries. And use the feature as target, and others as predictors. This method follows a strategy that consider each variable with missing values as the outcome and utilize the other complete column (filled with by the mean of respective columns for continuous and most frequent data for categorical type of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def impute(missing_df):\n",
    "    \n",
    "#     missing_reg_df = missing_df.copy()\n",
    "#     missing_reg_df_nan_count = missing_reg_df.isnull().sum()\n",
    "\n",
    "#     sort_index_li = np.argsort(missing_reg_df_nan_count).tolist()\n",
    "\n",
    "#     for i in sort_index_li:\n",
    "\n",
    "#         temp_df = missing_reg_df\n",
    "\n",
    "#         target = temp_df.iloc[:, i] # Get the target column\n",
    "\n",
    "#         temp_df = temp_df.iloc[:, temp_df.columns != i] # Kick it out of the data frame\n",
    "\n",
    "#         if target.name in num_li:\n",
    "#             temp_df_imputed = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(temp_df) # Fill by respective mean\n",
    "#         elif target.name in cat_li:\n",
    "#             temp_df_imputed = SimpleImputer(missing_values=np.nan, strategy='most_frequent').fit_transform(temp_df) # Fill by respective mode\n",
    "        \n",
    "#         # Select out the training set\n",
    "#         y_train = target[target.notnull()]\n",
    "#         x_train = temp_df_imputed[y_train.index, :]\n",
    "\n",
    "#         # Select out the test set\n",
    "#         y_test = target[target.isnull()]\n",
    "#         x_test = temp_df_imputed[y_test.index, :]\n",
    "\n",
    "#         model = RandomForestRegressor()\n",
    "#         model.fit(x_train, y_train)\n",
    "#         y_predict = model.predict(x_test)\n",
    "\n",
    "#         missing_reg_df.iloc[missing_reg_df.iloc[:, i].isnull(), i] = y_predict\n",
    "\n",
    "#     return missing_reg_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test():\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.df = df\n",
    "\n",
    "        self.performance, self.test_performance = None, None\n",
    "\n",
    "        self.ratio_list = [1/15, 1/10, 1/5]\n",
    "        mechans = [\"MNAR\", \"MCAR\", \"MAR\"]\n",
    "        self.metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "        \n",
    "        self.mechan = \"Default\"\n",
    "\n",
    "        self.mar_dep_col, self.mar_tar_col, self.mnar_tar_col = \"ca\", \"thal\", \"cp\"\n",
    "        self.ratio_dict = {\"ratio_mcar\": 1/30, \"ratio_mar\": 1/30, \"ratio_mnar\": 1/30}\n",
    "        \n",
    "        self.mul_idx = pd.MultiIndex.from_product(\n",
    "            [mechans, [f\"{r:.3f}\" for r in self.ratio_list]],\n",
    "             names=[\"mechans\", \"ratios\"]\n",
    "        )\n",
    "\n",
    "        # Baseline Performance\n",
    "        print(\"Initializing... (Mechan: Default)\")\n",
    "        self.get_dict()\n",
    "        self.preproc()\n",
    "        self.baseperformance()\n",
    "        self.insert_null() # Create missing values first\n",
    "        self.impute() # Impute the generated missing data frame\n",
    "        self.cv_eval()\n",
    "        self.test_eval()\n",
    "        print(\"Done\")\n",
    "    \n",
    "    def preproc(self):\n",
    "        df = self.df.copy()\n",
    "        df.loc[df[(df['num'] > 0)].index.tolist(),'num'] = 1\n",
    "\n",
    "        dropped_df = df.dropna().astype(np.float64)\n",
    "        num_cols = dropped_df[self.num_li]\n",
    "        num_cols = (num_cols - num_cols.min()) / (num_cols.max() - num_cols.min())\n",
    "        dropped_df.loc[:, self.num_li] = num_cols\n",
    "        # print(type(num_cols.iloc[0,0]))\n",
    "        self.dropped_df = dropped_df\n",
    "\n",
    "        X = self.dropped_df.drop('num', axis=1)\n",
    "        y = self.dropped_df['num']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                            random_state=42)\n",
    "\n",
    "        self.X_train = X_train.reset_index(drop=True)\n",
    "        self.X_test = X_test.reset_index(drop=True)\n",
    "        self.y_train = y_train.reset_index(drop=True)\n",
    "        self.y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    def get_dict(self):\n",
    "        df = self.df.iloc[:,:-1]\n",
    "        self.names = df.columns.to_list()\n",
    "\n",
    "        self.names_dict = {}\n",
    "        for i, n in enumerate(df.columns.to_list()): self.names_dict[n] = i\n",
    "            \n",
    "        self.num_li = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']\n",
    "        self.cat_li = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'thal']\n",
    "\n",
    "        self.num_dict = {n:self.names_dict[n] for n in self.num_li}\n",
    "        self.cat_dict = {c:self.names_dict[c] for c in self.cat_li}\n",
    "\n",
    "    def baseperformance(self, md=4, fold=10):\n",
    "        self.clf = xgb.XGBClassifier(max_depth=md)\n",
    "        \n",
    "        scores = cross_validate(self.clf, self.X_train,\n",
    "                                 self.y_train, cv=fold, \n",
    "                                 scoring=self.metrics)\n",
    "        \n",
    "        score_frame = pd.DataFrame(scores).iloc[:,2:]\n",
    "        avg_performance = score_frame.mean()\n",
    "\n",
    "        # print(pd.concat([score_frame, pd.DataFrame(avg_performance, columns=score_frame.columns)]))#, ignore_index=True))\n",
    "        print(f\"Score frame: \\n{score_frame}\")\n",
    "        print(f\"Mean of score frame: \\n{avg_performance}\")\n",
    "\n",
    "        self.clf.fit(self.X_train, self.y_train)\n",
    "\n",
    "        y_pred = self.clf.predict(self.X_test)\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        accuracy  = accuracy_score(self.y_test, y_pred)\n",
    "        precision = precision_score(self.y_test, y_pred)\n",
    "        recall    = recall_score(self.y_test, y_pred)\n",
    "        f1        = f1_score(self.y_test, y_pred)\n",
    "\n",
    "        print(f\"Baseline performance: Accurarcy: {accuracy * 100: .2f}%, Precision: {precision * 100: .2f}%, Recall: {recall * 100: .2f}%, F1: {f1 * 100: .2f}%.\")\n",
    "\n",
    "\n",
    "    def insert_mcar(self, ratio, df): # Input the percentage of mcar, and the data frame\n",
    "        \n",
    "        # Input the length and width of the whole data frame\n",
    "        def generate_pair(length, width): \n",
    "            # generate a pair of numbers, which is the location of the missing entry\n",
    "            return (np.random.randint(0, length), np.random.randint(0, width)) \n",
    "        \n",
    "        # Number of null values to insert\n",
    "        num_nulls = int(ratio * (df.size))\n",
    "\n",
    "        # Randomly choose positions to insert null values\n",
    "        idx_pair = []\n",
    "\n",
    "        for _ in range(num_nulls):\n",
    "            \n",
    "            p = generate_pair(df.shape[0], df.shape[1])\n",
    "            while p in idx_pair: p = generate_pair(df.shape[0], df.shape[1])\n",
    "            idx_pair.append(p)\n",
    "            \n",
    "            df.iat[p[0], p[1]] = np.nan\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def insert_mar(self, ratio, df, dep_col, tar_col): \n",
    "        \n",
    "        \"\"\"\n",
    "        Input:\n",
    "            - The percentage of data MAR to be generated\n",
    "            - The data frame\n",
    "            - Dependent column name: str\n",
    "            - Index of the target column to be inserted\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the index of entries to be remvoed\n",
    "        target_idx = df[dep_col] < df[dep_col].quantile(ratio)\n",
    "\n",
    "        # Remove the entries in the target column\n",
    "        df.iloc[target_idx, self.names_dict[tar_col]] = np.nan\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def insert_mnar(self, ratio, df, tar_col): \n",
    "\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            - The percentage of data MNAR to be generated\n",
    "            - The data frame\n",
    "            - Target column name: str\n",
    "            - Index of the target column to be inserted\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the index of entries to be remvoed\n",
    "        target_idx = df[tar_col] < df[tar_col].quantile(ratio)\n",
    "        \n",
    "        # Remove the entries in the target column\n",
    "        df.iloc[target_idx, self.names_dict[tar_col]] = np.nan\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def insert_null(self):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "                - The data frame to be inserted\n",
    "                - Ratios for each missing mechanism\n",
    "        \"\"\"\n",
    "        # self.mechan = mechan if mechan is not None else None\n",
    "        missing_df = self.X_train.copy()\n",
    "\n",
    "        # Example\n",
    "        missing_df = self.insert_mar(self.ratio_dict[\"ratio_mcar\"], missing_df, self.mar_dep_col, self.mar_tar_col)\n",
    "        missing_df = self.insert_mnar(self.ratio_dict[\"ratio_mnar\"], missing_df, self.mnar_tar_col)\n",
    "        self.missing_df = self.insert_mcar(self.ratio_dict[\"ratio_mar\"], missing_df)\n",
    "\n",
    "    def cv(self, md, fold):\n",
    "    \n",
    "        param = {\n",
    "            \"max_depth\": md, \n",
    "            \"eta\": 1, \n",
    "            \"objective\": \"binary:logistic\"\n",
    "        }\n",
    "\n",
    "        dtrain = xgb.DMatrix(\n",
    "            data=self.X_train,\n",
    "            label=self.y_train\n",
    "        )\n",
    "        \n",
    "        res = xgb.cv(\n",
    "            param,\n",
    "            dtrain,\n",
    "            num_boost_round=10,\n",
    "            nfold=fold,\n",
    "            metrics=(\"error\"),\n",
    "            seed=42,\n",
    "            as_pandas = True\n",
    "        )\n",
    "\n",
    "        return res\n",
    "\n",
    "    def impute(self):\n",
    "    \n",
    "        self.imputed_df = self.missing_df.copy()\n",
    "        \n",
    "        missing_df_nan_count = self.missing_df.isnull().sum()\n",
    "\n",
    "        sort_index_li = np.argsort(missing_df_nan_count).tolist()\n",
    "\n",
    "        # for i in tqdm(sort_index_li, desc=f\"Imputing with MNAR ratio: {self.ratio_dict['ratio_mnar']:.3f}, MAR ratio: {self.ratio_dict['ratio_mar']:.3f}, MCAR ratio: {self.ratio_dict['ratio_mcar']:.3f}\"):\n",
    "        for i in sort_index_li:\n",
    "            temp_df = self.imputed_df.copy()\n",
    "\n",
    "            target = temp_df.iloc[:, i] # Get the target column\n",
    "\n",
    "            temp_df = temp_df.iloc[:, temp_df.columns != i] # Kick it out of the data frame\n",
    "\n",
    "            if target.name in self.num_li:\n",
    "                self.num_li.remove(target.name)\n",
    "                temp_df[self.num_li] = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(temp_df[self.num_li]) # Fill by respective mean\n",
    "                temp_df_imputed = SimpleImputer(missing_values=np.nan, strategy='most_frequent').fit_transform(temp_df) # Fill by respective mode\n",
    "                self.num_li.append(target.name)\n",
    "            elif target.name in self.cat_li:\n",
    "                self.cat_li.remove(target.name)\n",
    "                temp_df[self.cat_li] = SimpleImputer(missing_values=np.nan, strategy='most_frequent').fit_transform(temp_df[self.cat_li])\n",
    "                temp_df_imputed = SimpleImputer(missing_values=np.nan, strategy='mean').fit_transform(temp_df)\n",
    "                self.cat_li.append(target.name)\n",
    "\n",
    "            # Select out the training set\n",
    "            y_train = target[target.notnull()]\n",
    "            x_train = temp_df_imputed[y_train.index, :]\n",
    "\n",
    "            # Select out the test set\n",
    "            y_test = target[target.isnull()]\n",
    "            x_test = temp_df_imputed[y_test.index, :]\n",
    "\n",
    "            self.imputer = RandomForestRegressor()\n",
    "            self.imputer.fit(x_train, y_train)\n",
    "            y_predict = self.imputer.predict(x_test)\n",
    "\n",
    "            self.imputed_df.iloc[y_test.index, i] = y_predict\n",
    "\n",
    "    \n",
    "    def cv_eval(self, ratio=0, md=4, fold=10):\n",
    "\n",
    "        self.clf = xgb.XGBClassifier(max_depth=md)\n",
    "        scores = cross_validate(self.clf, self.imputed_df,\n",
    "                                 self.y_train, cv=fold,\n",
    "                                 scoring=self.metrics)\n",
    "        \n",
    "        score_frame = pd.DataFrame(scores).iloc[:,2:]\n",
    "        self.avg_performance = score_frame.mean()\n",
    "\n",
    "        if self.performance is None and ratio==0:\n",
    "            self.performance = pd.DataFrame(np.nan, index=self.mul_idx, columns=[f'avg_cv_{m}' for m in self.metrics])\n",
    "            base_performance = pd.DataFrame({f'avg_cv_{m}':self.avg_performance[f'test_{m}'] for m in self.metrics}, index=[(\"DEFAULT\", f\"all {1/30:.3f}\")])\n",
    "            self.performance = pd.concat([self.performance, base_performance])                                \n",
    "        elif self.performance is not None and self.mechan is not None and ratio != 0:\n",
    "            for m in self.metrics:\n",
    "                self.performance.loc[pd.Index([(self.mechan, f\"{ratio:.3f}\")]), f'avg_cv_{m}'] = self.avg_performance[f'test_{m}']\n",
    "\n",
    "    def test_eval(self, ratio=0):\n",
    "        \n",
    "        self.clf.fit(self.imputed_df, self.y_train)\n",
    "        y_pred = self.clf.predict(self.X_test)\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        accuracy  = accuracy_score(self.y_test, y_pred)\n",
    "        precision = precision_score(self.y_test, y_pred)\n",
    "        recall    = recall_score(self.y_test, y_pred)\n",
    "        f1        = f1_score(self.y_test, y_pred)\n",
    "\n",
    "        # self.test_index = pd.Index([(f\"MNAR ratio: {self.ratio_dict['ratio_mnar']:.3f}, MAR ratio: {self.ratio_dict['ratio_mar']:.3f}, MCAR ratio: {self.ratio_dict['ratio_mcar']:.3f}\")])\n",
    "        values = np.array([accuracy, precision, recall, f1])#.reshape(1,4)\n",
    "\n",
    "        if self.test_performance is None:\n",
    "            self.test_performance = pd.DataFrame(np.nan, index=self.mul_idx, columns=[f'test_{m}' for m in self.metrics])\n",
    "            base_performance = pd.DataFrame({f'test_{m}':v for m, v in zip(self.metrics, values)}, index=[(\"DEFAULT\", f\"all {1/30:.3f}\")])\n",
    "            self.test_performance = pd.concat([self.test_performance, base_performance])\n",
    "        else:\n",
    "            self.test_performance.loc[pd.Index([(self.mechan, f\"{ratio:.3f}\")])] = values\n",
    "    \n",
    "    def forward(self, mechan):\n",
    "\n",
    "        self.mechan = mechan\n",
    "        \n",
    "        # for r in tqdm(self.ratio_list, desc=f\"Evaluating with {self.mechan}\"):\n",
    "        for r in self.ratio_list:\n",
    "\n",
    "            if self.mechan == \"MNAR\":\n",
    "                self.ratio_dict[\"ratio_mnar\"] = r\n",
    "            elif self.mechan == \"MAR\":\n",
    "                self.ratio_dict[\"ratio_mar\"] = r\n",
    "            elif self.mechan == \"MCAR\":\n",
    "                self.ratio_dict[\"ratio_mcar\"] = r\n",
    "\n",
    "            self.insert_null() # Create missing values first\n",
    "            self.impute() # Impute the generated missing data frame\n",
    "            self.cv_eval(r)\n",
    "        \n",
    "            # print(f\"Testing...\\n(mechan: {self.mechan}, ratio_mnar: {self.ratio_dict[\"ratio_mnar\"]:.3f}, ratio_mar: {self.ratio_dict[\"ratio_mar\"]:.3f}, ratio_mcar: {self.ratio_dict[\"ratio_mcar\"]:.3f})\")\n",
    "            self.test_eval(r) # Evaluate the model performance on test set\n",
    "            self.reset_ratio()\n",
    "        \n",
    "    def reset_ratio(self):\n",
    "        self.ratio_dict[\"ratio_mnar\"] = 1/30\n",
    "        self.ratio_dict[\"ratio_mar\"] = 1/30\n",
    "        self.ratio_dict[\"ratio_mcar\"] = 1/30\n",
    "\n",
    "    def forward_(self, mar_dep_col=\"ca\", mar_tar_col=\"thal\", mnar_tar_col=\"cp\"):\n",
    "\n",
    "        self.mar_dep_col, self.mar_tar_col, self.mnar_tar_col = mar_dep_col, mar_tar_col, mnar_tar_col\n",
    "        print(f\"Given mar_dep_col: {self.mar_dep_col}, mar_tar_col: {self.mar_tar_col}, mnar_tar_col: {self.mnar_tar_col}\")\n",
    "\n",
    "        # Evaluate the model performance on training set using CV\n",
    "        \n",
    "        # Start from MNAR\n",
    "        self.forward(mechan=\"MNAR\")\n",
    "\n",
    "        # Then MCAR\n",
    "        self.forward(mechan=\"MCAR\")\n",
    "\n",
    "        # Finally MAR\n",
    "        self.forward(mechan=\"MAR\")\n",
    "        \n",
    "        # self.performance.columns = [f'avg_cv_{m}' for m in self.metrics]\n",
    "        # print(self.performance)\n",
    "        # print(self.test_performance)\n",
    "\n",
    "    def res_format(self, df, caption, label):\n",
    "\n",
    "            s = df.style\n",
    "            \n",
    "            s = s.highlight_max(\n",
    "                props='bfseries:;'\n",
    "            ).format_index(escape=\"latex\", axis=1).set_table_styles([{'selector': 'caption','props': 'caption-side: bottom;'}], overwrite=False)\n",
    "\n",
    "            res = s.to_latex( \n",
    "                multirow_align=\"c\",\n",
    "                position_float=\"centering\",\n",
    "                clines=\"skip-last;data\",\n",
    "                hrules=True,\n",
    "                ).replace(\n",
    "                '\\\\end{tabular}',\n",
    "                '\\\\end{tabular}\\n' + '\\\\caption{' + f'{caption}' + '}\\n\\\\label{' + f'{label}' + '}')\n",
    "            \n",
    "            print(\"\".join(res.split(\"'\")))\n",
    "\n",
    "    def auto_forward(self, var1=\"ca\", var2=\"thal\", var3=\"cp\"):\n",
    "\n",
    "        vars = [var1, var2, var3]\n",
    "        iters = itertools.permutations(vars, 3)\n",
    "\n",
    "        for subset in tqdm(iters):\n",
    "            try:\n",
    "                self.forward_(subset[0], subset[1], subset[2])\n",
    "\n",
    "                print(\"Setting shown as follows:\\n Data MAR: \\\\texttt{\" + f\"{self.mar_dep_col}\"  + \"} determines \\\\texttt{\" + f\"{self.mar_tar_col}\"  + \"} \\& Variable MNAR: \\\\texttt{\" + f\"{self.mnar_tar_col}\" \\\n",
    "                + \"}.\\nTraining performance: See table \\\\ref{\" + f'trainpef_{self.mar_dep_col}_{self.mar_tar_col}_{self.mnar_tar_col}'\\\n",
    "                + \"}.\\nTest performance: See table \\\\ref{\" + f'testpef_{self.mar_dep_col}_{self.mar_tar_col}_{self.mnar_tar_col}' + \"}.\\n\")\n",
    "\n",
    "                self.res_format(self.performance, \"Training Performance of setting: MAR (\\\\texttt{\" + f\"{self.mar_dep_col}\"\\\n",
    "                                + \"} determines \\\\texttt{\" + f\"{self.mar_tar_col}\"  + \"}), \\\\texttt{\" + f\"{self.mnar_tar_col}\"\\\n",
    "                                + \"} MNAR\", f'trainpef_{self.mar_dep_col}_{self.mar_tar_col}_{self.mnar_tar_col}')\n",
    "                \n",
    "                self.res_format(self.test_performance, \"Test Performance of setting: MAR (\\\\texttt{\" + f\"{self.mar_dep_col}\"\\\n",
    "                                + \"} determines \\\\texttt{\" + f\"{self.mar_tar_col}\"  + \"}), \\\\texttt{\" + f\"{self.mnar_tar_col}\"\\\n",
    "                                + \"} MNAR\", f'testpef_{self.mar_dep_col}_{self.mar_tar_col}_{self.mnar_tar_col}')\n",
    "            except ValueError:\n",
    "                print(\"Something unusual happen. Please rerun.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing... (Mechan: Default)\n",
      "Score frame: \n",
      "   test_accuracy  test_precision  test_recall   test_f1\n",
      "0       0.833333        1.000000     0.636364  0.777778\n",
      "1       0.791667        0.800000     0.727273  0.761905\n",
      "2       0.791667        0.750000     0.818182  0.782609\n",
      "3       0.875000        0.833333     0.909091  0.869565\n",
      "4       0.791667        0.769231     0.833333  0.800000\n",
      "5       0.875000        0.909091     0.833333  0.869565\n",
      "6       0.666667        0.700000     0.583333  0.636364\n",
      "7       0.782609        0.750000     0.818182  0.782609\n",
      "8       0.869565        0.785714     1.000000  0.880000\n",
      "9       0.782609        1.000000     0.545455  0.705882\n",
      "Mean of score frame: \n",
      "test_accuracy     0.805978\n",
      "test_precision    0.829737\n",
      "test_recall       0.770455\n",
      "test_f1           0.786628\n",
      "dtype: float64\n",
      "Baseline performance: Accurarcy:  85.00%, Precision:  77.78%, Recall:  87.50%, F1:  82.35%.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "zhx = test(original_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.481132</td>\n",
       "      <td>0.244292</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.791667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.365297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.282443</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.241935</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.791667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.235160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.442748</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.283105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.885496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.564516</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.262557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.396947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.315068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.465649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.812500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.471698</td>\n",
       "      <td>0.152968</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.534351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.011416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.335878</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.251142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.786260</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age  sex   cp  trestbps      chol  fbs  restecg   thalach  exang  \\\n",
       "0    0.708333  1.0  1.0  0.481132  0.244292  1.0      2.0  0.603053    0.0   \n",
       "1    0.791667  1.0  4.0  0.622642  0.365297  0.0      2.0  0.282443    1.0   \n",
       "2    0.791667  1.0  4.0  0.245283  0.235160  0.0      2.0  0.442748    1.0   \n",
       "3    0.166667  1.0  3.0  0.339623  0.283105  0.0      0.0  0.885496    0.0   \n",
       "4    0.250000  0.0  2.0  0.339623  0.178082  0.0      2.0  0.770992    0.0   \n",
       "..        ...  ...  ...       ...       ...  ...      ...       ...    ...   \n",
       "297  0.583333  0.0  4.0  0.433962  0.262557  0.0      0.0  0.396947    1.0   \n",
       "298  0.333333  1.0  1.0  0.150943  0.315068  0.0      0.0  0.465649    0.0   \n",
       "299  0.812500  1.0  4.0  0.471698  0.152968  1.0      0.0  0.534351    0.0   \n",
       "300  0.583333  1.0  4.0  0.339623  0.011416  0.0      0.0  0.335878    1.0   \n",
       "301  0.583333  0.0  2.0  0.339623  0.251142  0.0      2.0  0.786260    0.0   \n",
       "\n",
       "      oldpeak  slope        ca  thal  num  \n",
       "0    0.370968    3.0  0.000000   6.0  0.0  \n",
       "1    0.241935    2.0  1.000000   3.0  1.0  \n",
       "2    0.419355    2.0  0.666667   7.0  1.0  \n",
       "3    0.564516    3.0  0.000000   3.0  0.0  \n",
       "4    0.225806    1.0  0.000000   3.0  0.0  \n",
       "..        ...    ...       ...   ...  ...  \n",
       "297  0.032258    2.0  0.000000   7.0  1.0  \n",
       "298  0.193548    2.0  0.000000   7.0  1.0  \n",
       "299  0.548387    2.0  0.666667   7.0  1.0  \n",
       "300  0.193548    2.0  0.333333   7.0  1.0  \n",
       "301  0.000000    2.0  0.333333   3.0  1.0  \n",
       "\n",
       "[297 rows x 14 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zhx.dropped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e451bd573d034b40abb8477e1de61a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given mar_dep_col: ca, mar_tar_col: thal, mnar_tar_col: age\n",
      "Setting shown as follows:\n",
      " Data MAR: \\texttt{ca} determines \\texttt{thal} \\& Variable MNAR: \\texttt{age}.\n",
      "Training performance: See table \\ref{trainpef_ca_thal_age}.\n",
      "Test performance: See table \\ref{testpef_ca_thal_age}.\n",
      "\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\begin{tabular}{llrrrr}\n",
      "\\toprule\n",
      " &  & avg\\_cv\\_accuracy & avg\\_cv\\_precision & avg\\_cv\\_recall & avg\\_cv\\_f1 \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{3}{*}{MNAR} & 0.067 & 0.801812 & 0.800004 & 0.778788 & \\bfseries 0.787596 \\\\\n",
      " & 0.100 & 0.793116 & 0.778013 & \\bfseries 0.796970 & 0.783915 \\\\\n",
      " & 0.200 & 0.785145 & 0.796851 & 0.743939 & 0.760310 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MCAR} & 0.067 & 0.784239 & 0.791492 & 0.750000 & 0.760028 \\\\\n",
      " & 0.100 & 0.779891 & 0.814246 & 0.717424 & 0.755285 \\\\\n",
      " & 0.200 & \\bfseries 0.809601 & \\bfseries 0.823898 & 0.777273 & 0.786702 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MAR} & 0.067 & 0.801087 & 0.808953 & 0.777273 & 0.786886 \\\\\n",
      " & 0.100 & 0.738406 & 0.734251 & 0.725758 & 0.719860 \\\\\n",
      " & 0.200 & 0.729891 & 0.714411 & 0.728788 & 0.716950 \\\\\n",
      "\\cline{1-6}\n",
      "base & all 0.033 & 0.793116 & 0.819910 & 0.742424 & 0.769534 \\\\\n",
      "\\cline{1-6}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{Training Performance of setting: MAR (\\texttt{ca} determines \\texttt{thal}), \\texttt{age} MNAR}\n",
      "\\label{trainpef_ca_thal_age}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\begin{tabular}{llrrrr}\n",
      "\\toprule\n",
      " &  & test\\_accuracy & test\\_precision & test\\_recall & test\\_f1 \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{3}{*}{MNAR} & 0.067 & 0.783333 & 0.720000 & 0.750000 & 0.734694 \\\\\n",
      " & 0.100 & 0.833333 & 0.769231 & 0.833333 & 0.800000 \\\\\n",
      " & 0.200 & \\bfseries 0.850000 & 0.758621 & \\bfseries 0.916667 & \\bfseries 0.830189 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MCAR} & 0.067 & 0.816667 & 0.740741 & 0.833333 & 0.784314 \\\\\n",
      " & 0.100 & 0.833333 & 0.769231 & 0.833333 & 0.800000 \\\\\n",
      " & 0.200 & \\bfseries 0.850000 & \\bfseries 0.777778 & 0.875000 & 0.823529 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MAR} & 0.067 & 0.800000 & 0.714286 & 0.833333 & 0.769231 \\\\\n",
      " & 0.100 & 0.816667 & 0.724138 & 0.875000 & 0.792453 \\\\\n",
      " & 0.200 & 0.800000 & 0.700000 & 0.875000 & 0.777778 \\\\\n",
      "\\cline{1-6}\n",
      "base & all 0.033 & 0.816667 & 0.740741 & 0.833333 & 0.784314 \\\\\n",
      "\\cline{1-6}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{Test Performance of setting: MAR (\\texttt{ca} determines \\texttt{thal}), \\texttt{age} MNAR}\n",
      "\\label{testpef_ca_thal_age}\n",
      "\\end{table}\n",
      "\n",
      "Given mar_dep_col: ca, mar_tar_col: age, mnar_tar_col: thal\n",
      "Setting shown as follows:\n",
      " Data MAR: \\texttt{ca} determines \\texttt{age} \\& Variable MNAR: \\texttt{thal}.\n",
      "Training performance: See table \\ref{trainpef_ca_age_thal}.\n",
      "Test performance: See table \\ref{testpef_ca_age_thal}.\n",
      "\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\begin{tabular}{llrrrr}\n",
      "\\toprule\n",
      " &  & avg\\_cv\\_accuracy & avg\\_cv\\_precision & avg\\_cv\\_recall & avg\\_cv\\_f1 \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{3}{*}{MNAR} & 0.067 & 0.776087 & 0.768666 & 0.770455 & 0.762609 \\\\\n",
      " & 0.100 & \\bfseries 0.818841 & 0.826930 & \\bfseries 0.796212 & \\bfseries 0.805266 \\\\\n",
      " & 0.200 & 0.809601 & \\bfseries 0.833838 & 0.761364 & 0.787892 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MCAR} & 0.067 & 0.772464 & 0.771064 & 0.743182 & 0.749526 \\\\\n",
      " & 0.100 & 0.809964 & 0.818566 & 0.787879 & 0.795239 \\\\\n",
      " & 0.200 & 0.809964 & 0.821353 & 0.779545 & 0.793105 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MAR} & 0.067 & 0.788587 & 0.802381 & 0.743939 & 0.768728 \\\\\n",
      " & 0.100 & 0.792935 & 0.795775 & 0.779545 & 0.782065 \\\\\n",
      " & 0.200 & 0.776449 & 0.774278 & 0.760606 & 0.760490 \\\\\n",
      "\\cline{1-6}\n",
      "base & all 0.033 & 0.793116 & 0.819910 & 0.742424 & 0.769534 \\\\\n",
      "\\cline{1-6}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{Training Performance of setting: MAR (\\texttt{ca} determines \\texttt{age}), \\texttt{thal} MNAR}\n",
      "\\label{trainpef_ca_age_thal}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\begin{tabular}{llrrrr}\n",
      "\\toprule\n",
      " &  & test\\_accuracy & test\\_precision & test\\_recall & test\\_f1 \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{3}{*}{MNAR} & 0.067 & \\bfseries 0.866667 & \\bfseries 0.807692 & 0.875000 & 0.840000 \\\\\n",
      " & 0.100 & 0.800000 & 0.714286 & 0.833333 & 0.769231 \\\\\n",
      " & 0.200 & 0.833333 & 0.791667 & 0.791667 & 0.791667 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MCAR} & 0.067 & 0.850000 & 0.777778 & 0.875000 & 0.823529 \\\\\n",
      " & 0.100 & 0.816667 & 0.740741 & 0.833333 & 0.784314 \\\\\n",
      " & 0.200 & \\bfseries 0.866667 & 0.785714 & \\bfseries 0.916667 & \\bfseries 0.846154 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MAR} & 0.067 & 0.816667 & 0.740741 & 0.833333 & 0.784314 \\\\\n",
      " & 0.100 & 0.783333 & 0.677419 & 0.875000 & 0.763636 \\\\\n",
      " & 0.200 & 0.816667 & 0.709677 & \\bfseries 0.916667 & 0.800000 \\\\\n",
      "\\cline{1-6}\n",
      "base & all 0.033 & 0.816667 & 0.740741 & 0.833333 & 0.784314 \\\\\n",
      "\\cline{1-6}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{Test Performance of setting: MAR (\\texttt{ca} determines \\texttt{age}), \\texttt{thal} MNAR}\n",
      "\\label{testpef_ca_age_thal}\n",
      "\\end{table}\n",
      "\n",
      "Given mar_dep_col: thal, mar_tar_col: ca, mnar_tar_col: age\n",
      "Setting shown as follows:\n",
      " Data MAR: \\texttt{thal} determines \\texttt{ca} \\& Variable MNAR: \\texttt{age}.\n",
      "Training performance: See table \\ref{trainpef_thal_ca_age}.\n",
      "Test performance: See table \\ref{testpef_thal_ca_age}.\n",
      "\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\begin{tabular}{llrrrr}\n",
      "\\toprule\n",
      " &  & avg\\_cv\\_accuracy & avg\\_cv\\_precision & avg\\_cv\\_recall & avg\\_cv\\_f1 \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{3}{*}{MNAR} & 0.067 & \\bfseries 0.805616 & 0.810556 & 0.786364 & \\bfseries 0.792911 \\\\\n",
      " & 0.100 & 0.784783 & 0.806798 & 0.734848 & 0.759746 \\\\\n",
      " & 0.200 & 0.797283 & 0.808887 & 0.758333 & 0.774155 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MCAR} & 0.067 & 0.788949 & 0.782774 & 0.786364 & 0.778547 \\\\\n",
      " & 0.100 & 0.767935 & 0.764909 & 0.753788 & 0.753006 \\\\\n",
      " & 0.200 & 0.784601 & 0.763834 & \\bfseries 0.795455 & 0.773939 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MAR} & 0.067 & 0.776087 & 0.786962 & 0.734848 & 0.752280 \\\\\n",
      " & 0.100 & 0.801449 & 0.808716 & 0.778030 & 0.787827 \\\\\n",
      " & 0.200 & 0.717391 & 0.732380 & 0.684091 & 0.696779 \\\\\n",
      "\\cline{1-6}\n",
      "base & all 0.033 & 0.793116 & \\bfseries 0.819910 & 0.742424 & 0.769534 \\\\\n",
      "\\cline{1-6}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{Training Performance of setting: MAR (\\texttt{thal} determines \\texttt{ca}), \\texttt{age} MNAR}\n",
      "\\label{trainpef_thal_ca_age}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\begin{tabular}{llrrrr}\n",
      "\\toprule\n",
      " &  & test\\_accuracy & test\\_precision & test\\_recall & test\\_f1 \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{3}{*}{MNAR} & 0.067 & 0.800000 & 0.714286 & 0.833333 & 0.769231 \\\\\n",
      " & 0.100 & 0.866667 & 0.785714 & \\bfseries 0.916667 & 0.846154 \\\\\n",
      " & 0.200 & 0.816667 & 0.740741 & 0.833333 & 0.784314 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MCAR} & 0.067 & 0.816667 & 0.760000 & 0.791667 & 0.775510 \\\\\n",
      " & 0.100 & 0.850000 & 0.777778 & 0.875000 & 0.823529 \\\\\n",
      " & 0.200 & 0.800000 & 0.730769 & 0.791667 & 0.760000 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MAR} & 0.067 & 0.816667 & 0.740741 & 0.833333 & 0.784314 \\\\\n",
      " & 0.100 & 0.850000 & 0.777778 & 0.875000 & 0.823529 \\\\\n",
      " & 0.200 & \\bfseries 0.883333 & \\bfseries 0.814815 & \\bfseries 0.916667 & \\bfseries 0.862745 \\\\\n",
      "\\cline{1-6}\n",
      "base & all 0.033 & 0.816667 & 0.740741 & 0.833333 & 0.784314 \\\\\n",
      "\\cline{1-6}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{Test Performance of setting: MAR (\\texttt{thal} determines \\texttt{ca}), \\texttt{age} MNAR}\n",
      "\\label{testpef_thal_ca_age}\n",
      "\\end{table}\n",
      "\n",
      "Given mar_dep_col: thal, mar_tar_col: age, mnar_tar_col: ca\n",
      "Setting shown as follows:\n",
      " Data MAR: \\texttt{thal} determines \\texttt{age} \\& Variable MNAR: \\texttt{ca}.\n",
      "Training performance: See table \\ref{trainpef_thal_age_ca}.\n",
      "Test performance: See table \\ref{testpef_thal_age_ca}.\n",
      "\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\begin{tabular}{llrrrr}\n",
      "\\toprule\n",
      " &  & avg\\_cv\\_accuracy & avg\\_cv\\_precision & avg\\_cv\\_recall & avg\\_cv\\_f1 \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{3}{*}{MNAR} & 0.067 & 0.810145 & 0.816606 & \\bfseries 0.787121 & 0.793144 \\\\\n",
      " & 0.100 & 0.814130 & 0.849509 & 0.760606 & 0.794613 \\\\\n",
      " & 0.200 & 0.775725 & 0.785628 & 0.759091 & 0.758956 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MCAR} & 0.067 & 0.792754 & 0.788833 & 0.777273 & 0.775754 \\\\\n",
      " & 0.100 & 0.809601 & 0.806803 & 0.785606 & 0.790822 \\\\\n",
      " & 0.200 & \\bfseries 0.830978 & \\bfseries 0.865527 & 0.786364 & \\bfseries 0.810402 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MAR} & 0.067 & 0.780616 & 0.774937 & 0.759848 & 0.763658 \\\\\n",
      " & 0.100 & 0.751812 & 0.768352 & 0.710606 & 0.727268 \\\\\n",
      " & 0.200 & 0.729891 & 0.746146 & 0.681061 & 0.706142 \\\\\n",
      "\\cline{1-6}\n",
      "base & all 0.033 & 0.793116 & 0.819910 & 0.742424 & 0.769534 \\\\\n",
      "\\cline{1-6}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{Training Performance of setting: MAR (\\texttt{thal} determines \\texttt{age}), \\texttt{ca} MNAR}\n",
      "\\label{trainpef_thal_age_ca}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\begin{tabular}{llrrrr}\n",
      "\\toprule\n",
      " &  & test\\_accuracy & test\\_precision & test\\_recall & test\\_f1 \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{3}{*}{MNAR} & 0.067 & 0.833333 & 0.733333 & \\bfseries 0.916667 & 0.814815 \\\\\n",
      " & 0.100 & \\bfseries 0.850000 & 0.758621 & \\bfseries 0.916667 & \\bfseries 0.830189 \\\\\n",
      " & 0.200 & \\bfseries 0.850000 & 0.777778 & 0.875000 & 0.823529 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MCAR} & 0.067 & 0.833333 & 0.769231 & 0.833333 & 0.800000 \\\\\n",
      " & 0.100 & 0.800000 & 0.730769 & 0.791667 & 0.760000 \\\\\n",
      " & 0.200 & \\bfseries 0.850000 & 0.777778 & 0.875000 & 0.823529 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MAR} & 0.067 & 0.833333 & \\bfseries 0.791667 & 0.791667 & 0.791667 \\\\\n",
      " & 0.100 & 0.833333 & 0.750000 & 0.875000 & 0.807692 \\\\\n",
      " & 0.200 & 0.816667 & 0.740741 & 0.833333 & 0.784314 \\\\\n",
      "\\cline{1-6}\n",
      "base & all 0.033 & 0.816667 & 0.740741 & 0.833333 & 0.784314 \\\\\n",
      "\\cline{1-6}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{Test Performance of setting: MAR (\\texttt{thal} determines \\texttt{age}), \\texttt{ca} MNAR}\n",
      "\\label{testpef_thal_age_ca}\n",
      "\\end{table}\n",
      "\n",
      "Given mar_dep_col: age, mar_tar_col: ca, mnar_tar_col: thal\n",
      "Setting shown as follows:\n",
      " Data MAR: \\texttt{age} determines \\texttt{ca} \\& Variable MNAR: \\texttt{thal}.\n",
      "Training performance: See table \\ref{trainpef_age_ca_thal}.\n",
      "Test performance: See table \\ref{testpef_age_ca_thal}.\n",
      "\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\begin{tabular}{llrrrr}\n",
      "\\toprule\n",
      " &  & avg\\_cv\\_accuracy & avg\\_cv\\_precision & avg\\_cv\\_recall & avg\\_cv\\_f1 \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{3}{*}{MNAR} & 0.067 & 0.759601 & 0.759538 & 0.725000 & 0.736878 \\\\\n",
      " & 0.100 & 0.775906 & 0.779376 & 0.743182 & 0.757299 \\\\\n",
      " & 0.200 & 0.792572 & 0.809019 & 0.752273 & 0.772624 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MCAR} & 0.067 & 0.792935 & 0.797463 & 0.771212 & 0.776230 \\\\\n",
      " & 0.100 & \\bfseries 0.797283 & \\bfseries 0.835671 & 0.752273 & 0.777256 \\\\\n",
      " & 0.200 & 0.793116 & 0.808245 & \\bfseries 0.778030 & \\bfseries 0.781032 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MAR} & 0.067 & 0.755435 & 0.757840 & 0.735606 & 0.737203 \\\\\n",
      " & 0.100 & 0.780435 & 0.773822 & 0.761364 & 0.762338 \\\\\n",
      " & 0.200 & 0.780435 & 0.789953 & 0.743939 & 0.761756 \\\\\n",
      "\\cline{1-6}\n",
      "base & all 0.033 & 0.793116 & 0.819910 & 0.742424 & 0.769534 \\\\\n",
      "\\cline{1-6}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{Training Performance of setting: MAR (\\texttt{age} determines \\texttt{ca}), \\texttt{thal} MNAR}\n",
      "\\label{trainpef_age_ca_thal}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\begin{tabular}{llrrrr}\n",
      "\\toprule\n",
      " &  & test\\_accuracy & test\\_precision & test\\_recall & test\\_f1 \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{3}{*}{MNAR} & 0.067 & 0.833333 & \\bfseries 0.791667 & 0.791667 & 0.791667 \\\\\n",
      " & 0.100 & 0.833333 & 0.750000 & 0.875000 & 0.807692 \\\\\n",
      " & 0.200 & 0.833333 & 0.769231 & 0.833333 & 0.800000 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MCAR} & 0.067 & 0.850000 & 0.777778 & 0.875000 & 0.823529 \\\\\n",
      " & 0.100 & 0.850000 & 0.777778 & 0.875000 & 0.823529 \\\\\n",
      " & 0.200 & 0.833333 & 0.769231 & 0.833333 & 0.800000 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MAR} & 0.067 & 0.850000 & 0.758621 & 0.916667 & 0.830189 \\\\\n",
      " & 0.100 & \\bfseries 0.866667 & 0.766667 & \\bfseries 0.958333 & \\bfseries 0.851852 \\\\\n",
      " & 0.200 & 0.833333 & 0.733333 & 0.916667 & 0.814815 \\\\\n",
      "\\cline{1-6}\n",
      "base & all 0.033 & 0.816667 & 0.740741 & 0.833333 & 0.784314 \\\\\n",
      "\\cline{1-6}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{Test Performance of setting: MAR (\\texttt{age} determines \\texttt{ca}), \\texttt{thal} MNAR}\n",
      "\\label{testpef_age_ca_thal}\n",
      "\\end{table}\n",
      "\n",
      "Given mar_dep_col: age, mar_tar_col: thal, mnar_tar_col: ca\n",
      "Setting shown as follows:\n",
      " Data MAR: \\texttt{age} determines \\texttt{thal} \\& Variable MNAR: \\texttt{ca}.\n",
      "Training performance: See table \\ref{trainpef_age_thal_ca}.\n",
      "Test performance: See table \\ref{testpef_age_thal_ca}.\n",
      "\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\begin{tabular}{llrrrr}\n",
      "\\toprule\n",
      " &  & avg\\_cv\\_accuracy & avg\\_cv\\_precision & avg\\_cv\\_recall & avg\\_cv\\_f1 \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{3}{*}{MNAR} & 0.067 & 0.780435 & 0.799557 & 0.743182 & 0.760931 \\\\\n",
      " & 0.100 & 0.767754 & 0.763287 & 0.743182 & 0.747758 \\\\\n",
      " & 0.200 & 0.801087 & 0.796854 & \\bfseries 0.786364 & 0.787391 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MCAR} & 0.067 & 0.805616 & 0.834108 & 0.751515 & 0.780296 \\\\\n",
      " & 0.100 & \\bfseries 0.818841 & \\bfseries 0.840564 & 0.780303 & \\bfseries 0.801231 \\\\\n",
      " & 0.200 & 0.776087 & 0.795345 & 0.733333 & 0.755238 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MAR} & 0.067 & 0.775725 & 0.778362 & 0.759848 & 0.762598 \\\\\n",
      " & 0.100 & 0.734420 & 0.724888 & 0.708333 & 0.711825 \\\\\n",
      " & 0.200 & 0.750725 & 0.764621 & 0.697727 & 0.723268 \\\\\n",
      "\\cline{1-6}\n",
      "base & all 0.033 & 0.793116 & 0.819910 & 0.742424 & 0.769534 \\\\\n",
      "\\cline{1-6}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{Training Performance of setting: MAR (\\texttt{age} determines \\texttt{thal}), \\texttt{ca} MNAR}\n",
      "\\label{trainpef_age_thal_ca}\n",
      "\\end{table}\n",
      "\n",
      "\\begin{table}\n",
      "\\centering\n",
      "\\begin{tabular}{llrrrr}\n",
      "\\toprule\n",
      " &  & test\\_accuracy & test\\_precision & test\\_recall & test\\_f1 \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{3}{*}{MNAR} & 0.067 & 0.850000 & 0.800000 & 0.833333 & 0.816327 \\\\\n",
      " & 0.100 & 0.833333 & 0.769231 & 0.833333 & 0.800000 \\\\\n",
      " & 0.200 & 0.816667 & 0.760000 & 0.791667 & 0.775510 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MCAR} & 0.067 & 0.850000 & 0.777778 & 0.875000 & 0.823529 \\\\\n",
      " & 0.100 & 0.833333 & 0.750000 & 0.875000 & 0.807692 \\\\\n",
      " & 0.200 & 0.800000 & 0.687500 & \\bfseries 0.916667 & 0.785714 \\\\\n",
      "\\cline{1-6}\n",
      "\\multirow[c]{3}{*}{MAR} & 0.067 & 0.833333 & 0.769231 & 0.833333 & 0.800000 \\\\\n",
      " & 0.100 & 0.866667 & 0.785714 & \\bfseries 0.916667 & 0.846154 \\\\\n",
      " & 0.200 & \\bfseries 0.900000 & \\bfseries 0.846154 & \\bfseries 0.916667 & \\bfseries 0.880000 \\\\\n",
      "\\cline{1-6}\n",
      "base & all 0.033 & 0.816667 & 0.740741 & 0.833333 & 0.784314 \\\\\n",
      "\\cline{1-6}\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\caption{Test Performance of setting: MAR (\\texttt{age} determines \\texttt{thal}), \\texttt{ca} MNAR}\n",
      "\\label{testpef_age_thal_ca}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zhx.auto_forward('ca', 'thal', 'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
